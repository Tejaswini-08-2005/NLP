{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPi/4GjY+5QwIAg9pnuhgYb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tejaswini-08-2005/NLP/blob/main/NLP_Assignment_8_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50ce5a65",
        "outputId": "489f5485-1596-4bf3-ea9d-688ca5ab98f6"
      },
      "source": [
        "corpus = [\n",
        "    'the quick brown fox jumps over the lazy dog',\n",
        "    'she sells seashells by the seashore',\n",
        "    'peter piper picked a peck of pickled peppers',\n",
        "    'how much wood would a woodchuck chuck',\n",
        "    'if a woodchuck could chuck wood'\n",
        "]\n",
        "\n",
        "print(\"Generated Corpus:\")\n",
        "for sentence in corpus:\n",
        "    print(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Corpus:\n",
            "the quick brown fox jumps over the lazy dog\n",
            "she sells seashells by the seashore\n",
            "peter piper picked a peck of pickled peppers\n",
            "how much wood would a woodchuck chuck\n",
            "if a woodchuck could chuck wood\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f3b66c8",
        "outputId": "bd8b9713-ef40-4beb-8e27-459740dadf97"
      },
      "source": [
        "corpus = [\n",
        "    'i am working in sr university',\n",
        "    'i did my phd in nitw',\n",
        "    'i did my masters in ouce',\n",
        "    'i am working as a professor',\n",
        "    'this is a test sentence'\n",
        "]\n",
        "print(\"Generated Corpus:\")\n",
        "for sentence in corpus:\n",
        "    print(sentence)\n",
        "tokens = [preprocess_sentence(s) for s in corpus]\n",
        "split = int(0.8 * len(tokens))\n",
        "train_data = tokens[:split]\n",
        "test_data = tokens[split:]\n",
        "print(f\"\\nTraining sentences: {len(train_data)}\")\n",
        "print(f\"Testing sentences: {len(test_data)}\")\n",
        "unigram_counts = build_ngram_model(train_data, 1)\n",
        "bigram_counts = build_ngram_model(train_data, 2)\n",
        "trigram_counts = build_ngram_model(train_data, 3)\n",
        "print(\"\\nUnigram Counts (top 10 including start/end tokens): \")\n",
        "for (word_tuple,), count in list(unigram_counts.items())[:10]:\n",
        "    print(f\"{word_tuple}: {count}\")\n",
        "print(\"\\nBigram Counts:\")\n",
        "for (w1, w2), count in bigram_counts.items():\n",
        "    print(f\"{w1} {w2}: {count}\")\n",
        "vocab = set([word[0] for word in unigram_counts if word[0] not in ['<s>', '</s>']])\n",
        "V = len(vocab) + 2\n",
        "print(f\"\\nVocabulary Size: {V}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Corpus:\n",
            "i am working in sr university\n",
            "i did my phd in nitw\n",
            "i did my masters in ouce\n",
            "i am working as a professor\n",
            "this is a test sentence\n",
            "\n",
            "Training sentences: 4\n",
            "Testing sentences: 1\n",
            "\n",
            "Unigram Counts (top 10 including start/end tokens): \n",
            "<s>: 4\n",
            "i: 4\n",
            "am: 2\n",
            "working: 2\n",
            "in: 3\n",
            "sr: 1\n",
            "university: 1\n",
            "</s>: 4\n",
            "did: 2\n",
            "my: 2\n",
            "\n",
            "Bigram Counts:\n",
            "<s> i: 4\n",
            "i am: 2\n",
            "am working: 2\n",
            "working in: 1\n",
            "in sr: 1\n",
            "sr university: 1\n",
            "university </s>: 1\n",
            "i did: 2\n",
            "did my: 2\n",
            "my phd: 1\n",
            "phd in: 1\n",
            "in nitw: 1\n",
            "nitw </s>: 1\n",
            "my masters: 1\n",
            "masters in: 1\n",
            "in ouce: 1\n",
            "ouce </s>: 1\n",
            "working as: 1\n",
            "as a: 1\n",
            "a professor: 1\n",
            "professor </s>: 1\n",
            "\n",
            "Vocabulary Size: 17\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Corpus constructed to match the given bigram counts\n",
        "Corpus = [\n",
        "    \"i am working as a professor i\",\n",
        "    \"am working in sr university i\",\n",
        "    \"i did my phd in nitw i\",\n",
        "    \"i did my masters in ouce\"\n",
        "]\n",
        "\n",
        "# Preprocess and tokenize\n",
        "tokens = []\n",
        "for sentence in Corpus:\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r'[^a-z\\s]', '', sentence)\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    tokens.extend(words)\n",
        "\n",
        "# Generate bigrams\n",
        "bigram_list = list(nltk.bigrams(tokens))\n",
        "\n",
        "# Count bigrams\n",
        "bigram_counts = Counter(bigram_list)\n",
        "\n",
        "# Display bigram counts\n",
        "print(\"Bigram Counts:\")\n",
        "for (w1, w2), count in bigram_counts.items():\n",
        "    print(f\"{w1} {w2}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsLLScoxH4wL",
        "outputId": "66cfab12-fc8f-4ed8-916e-1ffb038a0df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram Counts:\n",
            "i am: 2\n",
            "am working: 2\n",
            "working as: 1\n",
            "as a: 1\n",
            "a professor: 1\n",
            "professor i: 1\n",
            "working in: 1\n",
            "in sr: 1\n",
            "sr university: 1\n",
            "university i: 1\n",
            "i i: 2\n",
            "i did: 2\n",
            "did my: 2\n",
            "my phd: 1\n",
            "phd in: 1\n",
            "in nitw: 1\n",
            "nitw i: 1\n",
            "my masters: 1\n",
            "masters in: 1\n",
            "in ouce: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Same corpus used for unigram and bigram\n",
        "Corpus = [\n",
        "    \"i am working as a professor i\",\n",
        "    \"am working in sr university i\",\n",
        "    \"i did my phd in nitw i\",\n",
        "    \"i did my masters in ouce\"\n",
        "]\n",
        "\n",
        "# Preprocess and tokenize\n",
        "tokens = []\n",
        "for sentence in Corpus:\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r'[^a-z\\s]', '', sentence)\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    tokens.extend(words)\n",
        "\n",
        "# Generate trigrams\n",
        "trigram_list = list(nltk.trigrams(tokens))\n",
        "\n",
        "# Count trigrams\n",
        "trigram_counts = Counter(trigram_list)\n",
        "\n",
        "# Display trigram counts\n",
        "print(\"Trigram Counts:\")\n",
        "for (w1, w2, w3), count in trigram_counts.items():\n",
        "    print(f\"{w1} {w2} {w3}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VH_JBEK9Jkr7",
        "outputId": "7b726a3a-c9b8-4065-858f-67fe1565bdba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trigram Counts:\n",
            "i am working: 2\n",
            "am working as: 1\n",
            "working as a: 1\n",
            "as a professor: 1\n",
            "a professor i: 1\n",
            "professor i am: 1\n",
            "am working in: 1\n",
            "working in sr: 1\n",
            "in sr university: 1\n",
            "sr university i: 1\n",
            "university i i: 1\n",
            "i i did: 2\n",
            "i did my: 2\n",
            "did my phd: 1\n",
            "my phd in: 1\n",
            "phd in nitw: 1\n",
            "in nitw i: 1\n",
            "nitw i i: 1\n",
            "did my masters: 1\n",
            "my masters in: 1\n",
            "masters in ouce: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from collections import Counter\n",
        "nltk.download('punkt_tab')\n",
        "def preprocess_sentence(sentence):\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r'[^a-z\\s]', '', sentence)\n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    return ['<s>'] + words + ['</s>']\n",
        "def build_ngram_model(data, n):\n",
        "    ngrams = []\n",
        "    for sentence in data:\n",
        "        for i in range(len(sentence) - n + 1):\n",
        "            ngrams.append(tuple(sentence[i:i+n]))\n",
        "    return Counter(ngrams)\n",
        "def predict_next_word_bigram(word_sequence, bigram_counts, unigram_counts):\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "    last_word = words_in_sequence[-1]\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        if w1 == last_word:\n",
        "            potential_next_words[w2] = count\n",
        "    if not potential_next_words:\n",
        "        return f\"No bigram found starting with '{last_word}'.\"\n",
        "    last_word_unigram_count = unigram_counts.get(last_word, 0)\n",
        "    if last_word_unigram_count == 0:\n",
        "        return f\"'{last_word}' not found in unigram counts for probability calculation. Cannot predict next word.\"\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "    for next_word, bigram_count in potential_next_words.items():\n",
        "        probability = bigram_count / last_word_unigram_count\n",
        "        print(\"probability of  \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "    return predicted_word\n",
        "corpus = [\n",
        "    'i am working in sr university',\n",
        "    'i did my phd in nitw',\n",
        "    'i did my masters in ouce',\n",
        "    'i am working as a professor',\n",
        "    'this is a test sentence'\n",
        "]\n",
        "tokens = [preprocess_sentence(s) for s in corpus]\n",
        "split = int(0.8 * len(tokens))\n",
        "train_data = tokens[:split]\n",
        "test_data = tokens[split:]\n",
        "unigram_counts = build_ngram_model(train_data, 1)\n",
        "bigram_counts = build_ngram_model(train_data, 2)\n",
        "flat_unigram_counts = Counter()\n",
        "for (word_tuple,), count in unigram_counts.items():\n",
        "    flat_unigram_counts[word_tuple] = count\n",
        "sequence1 = \"I am\"\n",
        "next_word1 = predict_next_word_bigram(sequence1, bigram_counts, flat_unigram_counts)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_bigram(sequence2, bigram_counts, flat_unigram_counts)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")\n",
        "sequence3 = \"professor I\"\n",
        "next_word3 = predict_next_word_bigram(sequence3, bigram_counts, flat_unigram_counts)\n",
        "print(f\"Given sequence: '{sequence3}', predicted next word: '{next_word3}'\")\n",
        "sequence4 = \"nonexistent word\"\n",
        "next_word4 = predict_next_word_bigram(sequence4, bigram_counts, flat_unigram_counts)\n",
        "print(f\"Given sequence: '{sequence4}', predicted next word: '{next_word4}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0a1CmKnNb2c",
        "outputId": "be73ccc8-340f-4bcc-b091-b5439d883395"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of  working is  1.0\n",
            "Given sequence: 'I am', predicted next word: 'working'\n",
            "probability of  phd is  0.5\n",
            "probability of  masters is  0.5\n",
            "Given sequence: 'I did my', predicted next word: 'phd'\n",
            "probability of  am is  0.5\n",
            "probability of  did is  0.5\n",
            "Given sequence: 'professor I', predicted next word: 'am'\n",
            "Given sequence: 'nonexistent word', predicted next word: 'No bigram found starting with 'word'.'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_bigram(ip_text, bigram_counts, unigram_counts)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9QousPsOzZB",
        "outputId": "8492522f-feb8-4d6c-9396-7b2418884138"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter textI am\n",
            "Given sequence: 'I am', predicted next word: ''am' not found in unigram counts for probability calculation. Cannot predict next word.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_trigram(word_sequence, trigram_counts, bigram_counts):\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "    if len(words_in_sequence) < 2:\n",
        "        return \"Sequence must contain at least two words for trigram prediction.\"\n",
        "    last_two_words_tuple = tuple(words_in_sequence[-2:])\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2, w3), count in trigram_counts.items():\n",
        "        if (w1, w2) == last_two_words_tuple:\n",
        "            potential_next_words[w3] = count\n",
        "    if not potential_next_words:\n",
        "        return f\"No trigram found starting with '{' '.join(last_two_words_tuple)}'.\"\n",
        "    last_two_words_bigram_count = bigram_counts.get(last_two_words_tuple, 0)\n",
        "    if last_two_words_bigram_count == 0:\n",
        "        return f\"'{' '.join(last_two_words_tuple)}' not found as a bigram. Cannot predict next word.\"\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "    for next_word, trigram_count_val in potential_next_words.items(): # Renamed 'trigram_count' to 'trigram_count_val' to avoid shadowing the argument name\n",
        "        probability = trigram_count_val / last_two_words_bigram_count\n",
        "        print(\"probability of  \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "    return predicted_word\n",
        "\n",
        "# Assuming 'train_data' and 'bigram_counts' are available from previous cells (like Y0a1CmKnNb2c)\n",
        "# We need to define trigram_counts here for the given corpus.\n",
        "trigram_counts = build_ngram_model(train_data, 3)\n",
        "\n",
        "sequence1 = \"I am working\"\n",
        "next_word1 = predict_next_word_trigram(sequence1, trigram_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_trigram(sequence2, trigram_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGfPXgPTPKa7",
        "outputId": "7d4d177c-79da-425a-94e8-0ffa1034d2a3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of  in is  0.5\n",
            "probability of  as is  0.5\n",
            "Given sequence: 'I am working', predicted next word: 'in'\n",
            "probability of  phd is  0.5\n",
            "probability of  masters is  0.5\n",
            "Given sequence: 'I did my', predicted next word: 'phd'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_trigram(ip_text, trigram_counts, bigram_counts)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKsytfz_P1Zg",
        "outputId": "84b029e8-8cc0-4547-d50f-0aa28d61ab8b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter texti want\n",
            "Given sequence: 'i want', predicted next word: 'No trigram found starting with 'i want'.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_bigram_Laplace(word_sequence, bigram_counts, unigram_counts_for_laplace):\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "    last_word = words_in_sequence[-1]\n",
        "\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        if w1 == last_word:\n",
        "            potential_next_words[w2] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No bigram found starting with '{last_word}'.\"\n",
        "\n",
        "    last_word_unigram_count = unigram_counts_for_laplace.get(last_word, 0)\n",
        "    # V is the vocabulary size, including start/end tokens, for Laplace smoothing\n",
        "    # It needs to be defined in the global scope or passed to the function\n",
        "    # For this example, we'll re-calculate it based on the flat_unigram_counts from the current corpus\n",
        "\n",
        "    # Calculate V (vocabulary size) locally for this example's context\n",
        "    # This should match the V used in the other probability functions.\n",
        "    vocab_for_laplace = set(unigram_counts_for_laplace.keys()) # Includes <s> and </s>\n",
        "    V = len(vocab_for_laplace)\n",
        "\n",
        "    if last_word_unigram_count == 0:\n",
        "        # If the unigram count for the last word is 0, the denominator for the smoothing would be V.\n",
        "        # This would lead to all next words having the probability 1/V for bigram counts of 0.\n",
        "        # However, for words not seen as unigrams at all, predicting is harder.\n",
        "        # For this function, we'll return a message if the last_word itself was not seen.\n",
        "        return f\"'{last_word}' not found in unigram counts for Laplace smoothing. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, bigram_count in potential_next_words.items():\n",
        "        probability = (bigram_count + 1) / (last_word_unigram_count + V)\n",
        "        print(\"probability of \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Ensure flat_unigram_counts and bigram_counts are available from previous cells\n",
        "# (They are created in cell Y0a1CmKnNb2c)\n",
        "\n",
        "# Recalculate V for the specific corpus and flat_unigram_counts for Laplace smoothing\n",
        "vocab_for_laplace = set(flat_unigram_counts.keys())\n",
        "V = len(vocab_for_laplace)\n",
        "\n",
        "sequence1 = \"I am\"\n",
        "next_word1 = predict_next_word_bigram_Laplace(sequence1, bigram_counts, flat_unigram_counts)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_bigram_Laplace(sequence2, bigram_counts, flat_unigram_counts)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")\n",
        "\n",
        "sequence3 = \"professor I\"\n",
        "next_word3 = predict_next_word_bigram_Laplace(sequence3, bigram_counts, flat_unigram_counts)\n",
        "print(f\"Given sequence: '{sequence3}', predicted next word: '{next_word3}'\")\n",
        "\n",
        "sequence4 = \"nonexistent word\"\n",
        "next_word4 = predict_next_word_bigram_Laplace(sequence4, bigram_counts, flat_unigram_counts)\n",
        "print(f\"Given sequence: '{sequence4}', predicted next word: '{next_word4}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhl67lgOQQ1X",
        "outputId": "39c55b83-193c-4d3f-b85e-e5ee0028c1e5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of working is  0.15789473684210525\n",
            "Given sequence: 'I am', predicted next word: 'working'\n",
            "probability of phd is  0.10526315789473684\n",
            "probability of masters is  0.10526315789473684\n",
            "Given sequence: 'I did my', predicted next word: 'phd'\n",
            "probability of am is  0.14285714285714285\n",
            "probability of did is  0.14285714285714285\n",
            "Given sequence: 'professor I', predicted next word: 'am'\n",
            "Given sequence: 'nonexistent word', predicted next word: 'No bigram found starting with 'word'.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "next_word1 = predict_next_word_bigram_Laplace(ip_text, bigram_counts, flat_unigram_counts)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kSDWe45QuX9",
        "outputId": "0abb3879-218a-4191-b1c4-d785d3dff9e1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter texti completed\n",
            "Given sequence: 'i completed', predicted next word: 'No bigram found starting with 'completed'.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_trigram_Laplace(word_sequence, trigram_counts, bigram_counts, V_size):\n",
        "    # Tokenize the input sequence\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "\n",
        "    # Ensure at least two words for trigram prediction\n",
        "    if len(words_in_sequence) < 2:\n",
        "        return \"Sequence must contain at least two words for trigram prediction.\"\n",
        "\n",
        "    # Get the last two words as a tuple\n",
        "    last_two_words_tuple = tuple(words_in_sequence[-2:])\n",
        "\n",
        "    # Find potential next words based on trigrams starting with the last two words\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2, w3), count in trigram_counts.items():\n",
        "        if (w1, w2) == last_two_words_tuple:\n",
        "            potential_next_words[w3] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No trigram found starting with '{' '.join(last_two_words_tuple)}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w3 | w1,w2) = (Count(w1, w2, w3) + 1) / (Count(w1, w2) + V_size)\n",
        "    last_two_words_bigram_count = bigram_counts.get(last_two_words_tuple, 0)\n",
        "    if last_two_words_bigram_count == 0:\n",
        "        # If the bigram count for the last two words is 0, apply smoothing for the denominator too\n",
        "        # This ensures we don't divide by zero and can still compute a small probability\n",
        "        return f\"'{' '.join(last_two_words_tuple)}' not found as a bigram. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, trigram_count_val in potential_next_words.items():\n",
        "        probability = (trigram_count_val + 1) / (last_two_words_bigram_count + V_size)\n",
        "        print(\"probability of  \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Ensure bigram_counts, unigram_counts, and trigram_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Define trigram_counts and V for Laplace smoothing in this scope\n",
        "trigram_counts = build_ngram_model(train_data, 3)\n",
        "vocab_for_laplace = set(flat_unigram_counts.keys())\n",
        "V = len(vocab_for_laplace)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am working\"\n",
        "next_word1 = predict_next_word_trigram_Laplace(sequence1, trigram_counts, bigram_counts, V)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_trigram_Laplace(sequence2, trigram_counts, bigram_counts, V)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbuASnS_RvtF",
        "outputId": "08985ce5-fcce-4b72-a78d-b5fb9a9b94e0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of  in is  0.10526315789473684\n",
            "probability of  as is  0.10526315789473684\n",
            "Given sequence: 'I am working', predicted next word: 'in'\n",
            "probability of  phd is  0.10526315789473684\n",
            "probability of  masters is  0.10526315789473684\n",
            "Given sequence: 'I did my', predicted next word: 'phd'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m"
      ],
      "metadata": {
        "id": "G_0JMyHfSX9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "\n",
        "# Ensure trigram_counts and V are defined for this cell's scope\n",
        "# These should be available from previous cells, but we re-declare them for clarity/safety.\n",
        "trigram_counts = build_ngram_model(train_data, 3)\n",
        "vocab_for_laplace = set(flat_unigram_counts.keys())\n",
        "V = len(vocab_for_laplace)\n",
        "\n",
        "next_word1 = predict_next_word_trigram_Laplace(ip_text, trigram_counts, bigram_counts, V)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRiFolBgSD0X",
        "outputId": "6417be71-c6e8-4a91-e022-e20632c5017d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter textiam\n",
            "Given sequence: 'iam', predicted next word: 'Sequence must contain at least two words for trigram prediction.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_bigram_K(word_sequence, bigram_counts, unigram_counts_for_k, K): #K=0.5-0.01\n",
        "    # Tokenize the input sequence and get the last word\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "    last_word = words_in_sequence[-1]\n",
        "\n",
        "    # Find potential next words based on bigrams starting with last_word\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2), count in bigram_counts.items():\n",
        "        if w1 == last_word:\n",
        "            potential_next_words[w2] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No bigram found starting with '{last_word}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w2 | w1) = Count(w1, w2) / Count(w1)\n",
        "    last_word_unigram_count = unigram_counts_for_k.get(last_word, 0)\n",
        "\n",
        "    # Calculate V (vocabulary size) locally for this example's context\n",
        "    vocab_for_k = set(unigram_counts_for_k.keys())\n",
        "    V = len(vocab_for_k)\n",
        "\n",
        "    if last_word_unigram_count == 0:\n",
        "        return f\"'{last_word}' not found in unigram counts. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, bigram_count in potential_next_words.items():\n",
        "        probability = (bigram_count+K) / (last_word_unigram_count+K*V)\n",
        "        print(\"probability of \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts and flat_unigram_counts are defined from previous cells\n",
        "# (They are present in the kernel state, from cell Y0a1CmKnNb2c)\n",
        "\n",
        "# Recalculate V for the specific corpus and flat_unigram_counts for K-smoothing\n",
        "vocab_for_k_smoothing = set(flat_unigram_counts.keys())\n",
        "V = len(vocab_for_k_smoothing)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am\"\n",
        "next_word1 = predict_next_word_bigram_K(sequence1, bigram_counts, flat_unigram_counts, 0.5)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_bigram_K(sequence2, bigram_counts, flat_unigram_counts, 0.5)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")\n",
        "\n",
        "sequence3 = \"professor I\"\n",
        "next_word3 = predict_next_word_bigram_K(sequence3, bigram_counts, flat_unigram_counts, 0.5)\n",
        "print(f\"Given sequence: '{sequence3}', predicted next word: '{next_word3}'\")\n",
        "\n",
        "sequence4 = \"nonexistent word\"\n",
        "next_word4 = predict_next_word_bigram_K(sequence4, bigram_counts, flat_unigram_counts, 0.5)\n",
        "print(f\"Given sequence: '{sequence4}', predicted next word: '{next_word4}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDtf-vNXSbCX",
        "outputId": "c038f556-3535-43fb-f869-e25e5311a070"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of working is  0.23809523809523808\n",
            "Given sequence: 'I am', predicted next word: 'working'\n",
            "probability of phd is  0.14285714285714285\n",
            "probability of masters is  0.14285714285714285\n",
            "Given sequence: 'I did my', predicted next word: 'phd'\n",
            "probability of am is  0.2\n",
            "probability of did is  0.2\n",
            "Given sequence: 'professor I', predicted next word: 'am'\n",
            "Given sequence: 'nonexistent word', predicted next word: 'No bigram found starting with 'word'.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ip_text=input(\"enter text\")\n",
        "# Ensure V is defined for this cell's scope for K-smoothing, if not already global.\n",
        "# It's calculated in dDtf-vNXSbCX and hhl67lgOQQ1X. Assuming it's available.\n",
        "\n",
        "next_word1 = predict_next_word_bigram_K(ip_text, bigram_counts, flat_unigram_counts, 0.5)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word1}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMzDnQnhTFdJ",
        "outputId": "46e04b57-a228-4d41-8aac-b537dd6a579b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter texti completed\n",
            "Given sequence: 'i completed', predicted next word: 'No bigram found starting with 'completed'.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word_trigram_K(word_sequence, trigram_counts, bigram_counts,K): #K=0.5-0.01\n",
        "    # Tokenize the input sequence\n",
        "    words_in_sequence = word_sequence.lower().split()\n",
        "    if not words_in_sequence:\n",
        "        return \"Please provide a word sequence.\"\n",
        "\n",
        "    # Ensure at least two words for trigram prediction\n",
        "    if len(words_in_sequence) < 2:\n",
        "        return \"Sequence must contain at least two words for trigram prediction.\"\n",
        "\n",
        "    # Get the last two words as a tuple\n",
        "    last_two_words_tuple = tuple(words_in_sequence[-2:])\n",
        "\n",
        "    # Find potential next words based on trigrams starting with the last two words\n",
        "    potential_next_words = {}\n",
        "    for (w1, w2, w3), count in trigram_counts.items():\n",
        "        if (w1, w2) == last_two_words_tuple:\n",
        "            potential_next_words[w3] = count\n",
        "\n",
        "    if not potential_next_words:\n",
        "        return f\"No trigram found starting with '{' '.join(last_two_words_tuple)}'.\"\n",
        "\n",
        "    # Calculate probabilities for potential next words\n",
        "    # P(w3 | w1,w2) = Count(w1, w2, w3) / Count(w1, w2)\n",
        "    # The denominator should be the count of the bigram (w1, w2)\n",
        "    last_two_words_bigram_count = bigram_counts.get(last_two_words_tuple, 0)\n",
        "    if last_two_words_bigram_count == 0:\n",
        "        return f\"'{' '.join(last_two_words_tuple)}' not found as a bigram. Cannot predict next word.\"\n",
        "\n",
        "    predicted_word = None\n",
        "    max_probability = -1\n",
        "\n",
        "    for next_word, trigram_count in potential_next_words.items():\n",
        "        probability = (trigram_count+K) / (last_two_words_bigram_count+K*V)\n",
        "        print(\"probability of  \" f'{next_word}' \" is \", probability)\n",
        "        if probability > max_probability:\n",
        "            max_probability = probability\n",
        "            predicted_word = next_word\n",
        "\n",
        "    return predicted_word\n",
        "\n",
        "# Example usage:\n",
        "# Ensure bigram_counts, unigram_counts, and Trigrams_counts are defined from previous cells\n",
        "# (They are present in the kernel state.)\n",
        "\n",
        "# Try with a word sequence\n",
        "sequence1 = \"I am working\"\n",
        "next_word1 = predict_next_word_trigram_K(sequence1, trigram_counts, bigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence1}', predicted next word: '{next_word1}'\")\n",
        "\n",
        "sequence2 = \"I did my\"\n",
        "next_word2 = predict_next_word_trigram_K(sequence2, trigram_counts, bigram_counts,0.5)\n",
        "print(f\"Given sequence: '{sequence2}', predicted next word: '{next_word2}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egWMX3g7TUuf",
        "outputId": "c31a35cc-dbda-4220-a6d4-8333a9b6a84e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probability of  in is  0.14285714285714285\n",
            "probability of  as is  0.14285714285714285\n",
            "Given sequence: 'I am working', predicted next word: 'in'\n",
            "probability of  phd is  0.14285714285714285\n",
            "probability of  masters is  0.14285714285714285\n",
            "Given sequence: 'I did my', predicted next word: 'phd'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75295b25",
        "outputId": "b370f751-8922-4d08-9be7-7bf0f13eb481"
      },
      "source": [
        "ip_text = \"i am\"\n",
        "# Ensure V is defined (it's defined in cell dDtf-vNXSbCX, assuming it's available in kernel state)\n",
        "# For safety and clarity, re-defining V if it might be missing in a fresh execution context\n",
        "# (though in Colab it often persists if the notebook runs linearly)\n",
        "vocab_for_k_smoothing = set(flat_unigram_counts.keys())\n",
        "V = len(vocab_for_k_smoothing)\n",
        "\n",
        "print(f\"--- Prediction for '{ip_text}' with current corpus (K-smoothing) ---\")\n",
        "next_word_prediction = predict_next_word_bigram_K(ip_text, bigram_counts, flat_unigram_counts, 0.5)\n",
        "print(f\"Given sequence: '{ip_text}', predicted next word: '{next_word_prediction}'\")\n",
        "print(\"---\")\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Prediction for 'i am' with current corpus (K-smoothing) ---\n",
            "probability of working is  0.23809523809523808\n",
            "Given sequence: 'i am', predicted next word: 'working'\n",
            "---\n"
          ]
        }
      ]
    }
  ]
}