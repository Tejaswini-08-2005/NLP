{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXL7vLwQyx4sThk25Snt9U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tejaswini-08-2005/NLP/blob/main/NLP_Assignment_3_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task #1"
      ],
      "metadata": {
        "id": "mBKMRRqrwXRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "sentence = \"Students are learning Natural Language Processing\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1gPFNZIu1xf",
        "outputId": "549a954d-2cbe-4bba-cb59-20305c47aeea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Students', 'NNS'), ('are', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Students are learning Natural Language Processing\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K__jWL3tu5uz",
        "outputId": "90a755fd-0696-4833-c75e-d650db12c012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Students NOUN\n",
            "are AUX\n",
            "learning VERB\n",
            "Natural PROPN\n",
            "Language PROPN\n",
            "Processing NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying a startup in India.\")\n",
        "for token in doc:\n",
        "  print (token.text, token.pos_, token.tag_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFbLoqI3u_Lv",
        "outputId": "457859e7-3450-454f-96f0-2df2907e3a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple PROPN NNP\n",
            "is AUX VBZ\n",
            "looking VERB VBG\n",
            "at ADP IN\n",
            "buying VERB VBG\n",
            "a DET DT\n",
            "startup NOUN NN\n",
            "in ADP IN\n",
            "India PROPN NNP\n",
            ". PUNCT .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text =  \"Loving the new AI features  #AI #MachineLearning\"\n",
        "doc = nlp(text)\n",
        "nouns = []\n",
        "verbs = []\n",
        "for token in doc:\n",
        "    if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
        "        nouns.append(token.text)\n",
        "    elif token.pos_ == \"VERB\":\n",
        "        verbs.append(token.text)\n",
        "noun_freq = Counter(nouns)\n",
        "verb_freq = Counter(verbs)\n",
        "print(\"Noun Frequency:\", noun_freq)\n",
        "print(\"Verb Frequency:\", verb_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-NNXlI-vT7X",
        "outputId": "95157a73-84bd-4529-f787-63d98c9162aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun Frequency: Counter({'AI': 2, 'MachineLearning': 1})\n",
            "Verb Frequency: Counter({'Loving': 1, 'features': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TASK #02"
      ],
      "metadata": {
        "id": "fLI3_71vs3KC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMfBYC8ynYoE",
        "outputId": "c71c8dc0-0a6f-4285-fdaa-7a305aaebfb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Students', 'NNS'), ('are', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP')]\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "sentence = \"Students are learning Natural Language Processing\"\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Students are learning Natural Language Processing\")\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9jGpnDHp1Ev",
        "outputId": "1ba31d13-fb5f-46ce-f5a1-41d7011c678d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Students NOUN\n",
            "are AUX\n",
            "learning VERB\n",
            "Natural PROPN\n",
            "Language PROPN\n",
            "Processing NOUN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying a startup in India.\")\n",
        "for token in doc:\n",
        "  print (token.text, token.pos_, token.tag_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQI6_-ISqIi5",
        "outputId": "2093c87e-cf6c-4ca7-f7ca-605753849e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple PROPN NNP\n",
            "is AUX VBZ\n",
            "looking VERB VBG\n",
            "at ADP IN\n",
            "buying VERB VBG\n",
            "a DET DT\n",
            "startup NOUN NN\n",
            "in ADP IN\n",
            "India PROPN NNP\n",
            ". PUNCT .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"sr university warangal tejaswini book \"\n",
        "doc = nlp(text)\n",
        "nouns = []\n",
        "verbs = []\n",
        "for token in doc:\n",
        "    if token.pos_ in [\"NOUN\", \"PROPN\"]:\n",
        "        nouns.append(token.text)\n",
        "    elif token.pos_ == \"VERB\":\n",
        "        verbs.append(token.text)\n",
        "noun_freq = Counter(nouns)\n",
        "verb_freq = Counter(verbs)\n",
        "print(\"Noun Frequency:\", noun_freq)\n",
        "print(\"Verb Frequency:\", verb_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ayd476S0qP9v",
        "outputId": "747c1815-76fa-46cc-f36b-eaf6b8657dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noun Frequency: Counter({'sr': 1, 'university': 1, 'warangal': 1, 'tejaswini': 1, 'book': 1})\n",
            "Verb Frequency: Counter()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task #3"
      ],
      "metadata": {
        "id": "85mQQPj_wbr0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS Tagging in Academic Writing: Structural and Vocabulary Analysis"
      ],
      "metadata": {
        "id": "A7kblIW-CJRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aim\n",
        "\n",
        "To analyze the structure and vocabulary of academic writing by applying Part-of-Speech (POS) tagging using NLTK and spaCy, and to compare how both tools represent academic language."
      ],
      "metadata": {
        "id": "rYgYJ1mtCe2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import Libraries and Setup"
      ],
      "metadata": {
        "id": "n-M0cGIfCrud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install nltk spacy\n",
        "# Download spaCy English model\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh1MsRhEB0wt",
        "outputId": "87b431d6-3250-4667-bdae-366878814d0d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.21.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLL3KRrXC8ZG",
        "outputId": "5f74235d-0786-494e-d7a8-ff2bf412069c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Load and Tokenize Essay Text\n",
        "\n",
        "Input Academic Essay Text\n",
        "\n",
        "(Replace this with your actual essay)"
      ],
      "metadata": {
        "id": "hfR3uqeEDZ81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "essay_text = \"\"\"Academic writing requires clarity, precision, and critical analysis.\n",
        "Researchers argue that effective argumentation strengthens scholarly communication.\n",
        "This study examines linguistic patterns in academic discourse.\"\"\"\n"
      ],
      "metadata": {
        "id": "eDCOyjAQDemm"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization (NLTK)"
      ],
      "metadata": {
        "id": "gtyTZZM4EGim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize(essay_text)\n",
        "tokens[:20]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVJkYY7qEJlR",
        "outputId": "e28e5a9a-5fc4-4b2c-ea14-a73e1083c493"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Academic',\n",
              " 'writing',\n",
              " 'requires',\n",
              " 'clarity',\n",
              " ',',\n",
              " 'precision',\n",
              " ',',\n",
              " 'and',\n",
              " 'critical',\n",
              " 'analysis',\n",
              " '.',\n",
              " 'Researchers',\n",
              " 'argue',\n",
              " 'that',\n",
              " 'effective',\n",
              " 'argumentation',\n",
              " 'strengthens',\n",
              " 'scholarly',\n",
              " 'communication',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. POS Tagging Using NLTK"
      ],
      "metadata": {
        "id": "bJvZqKeGESeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_pos_tags = nltk.pos_tag(tokens)\n",
        "nltk_pos_tags[:15]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4HBSZSxEXee",
        "outputId": "5c15b36e-33a8-48fe-ed4d-d86340c6ea58"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Academic', 'NNP'),\n",
              " ('writing', 'NN'),\n",
              " ('requires', 'VBZ'),\n",
              " ('clarity', 'NN'),\n",
              " (',', ','),\n",
              " ('precision', 'NN'),\n",
              " (',', ','),\n",
              " ('and', 'CC'),\n",
              " ('critical', 'JJ'),\n",
              " ('analysis', 'NN'),\n",
              " ('.', '.'),\n",
              " ('Researchers', 'NNP'),\n",
              " ('argue', 'VBP'),\n",
              " ('that', 'IN'),\n",
              " ('effective', 'JJ')]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK Tag Set Example\n",
        "\n",
        "NN / NNS → Nouns (concepts)\n",
        "\n",
        "VB / VBD / VBG / VBN / VBP / VBZ → Verbs (arguments)\n",
        "\n",
        "JJ → Adjectives (evaluation)"
      ],
      "metadata": {
        "id": "4W0WB9s6EhJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. POS Tagging Using spaCy"
      ],
      "metadata": {
        "id": "6IFeY95REnEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(essay_text)\n",
        "spacy_pos_tags = [(token.text, token.pos_, token.tag_) for token in doc]\n",
        "spacy_pos_tags[:15]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X8s8A_8Eiw7",
        "outputId": "94cd2ec3-dc6f-4d8a-db51-bb6a5f59fd10"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Academic', 'ADJ', 'JJ'),\n",
              " ('writing', 'NOUN', 'NN'),\n",
              " ('requires', 'VERB', 'VBZ'),\n",
              " ('clarity', 'NOUN', 'NN'),\n",
              " (',', 'PUNCT', ','),\n",
              " ('precision', 'NOUN', 'NN'),\n",
              " (',', 'PUNCT', ','),\n",
              " ('and', 'CCONJ', 'CC'),\n",
              " ('critical', 'ADJ', 'JJ'),\n",
              " ('analysis', 'NOUN', 'NN'),\n",
              " ('.', 'PUNCT', '.'),\n",
              " ('\\n', 'SPACE', '_SP'),\n",
              " ('Researchers', 'NOUN', 'NNS'),\n",
              " ('argue', 'VERB', 'VBP'),\n",
              " ('that', 'SCONJ', 'IN')]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy Tag Set Example\n",
        "\n",
        "NOUN / PROPN → Concepts\n",
        "\n",
        "VERB / AUX → Arguments\n",
        "\n",
        "ADJ → Evaluation\n",
        "\n",
        "ADV → Hedging / stance"
      ],
      "metadata": {
        "id": "3fJLZXiQEuXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Comparison of NLTK vs spaCy POS Tag Sets\n"
      ],
      "metadata": {
        "id": "iunK4jUTE1RM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Feature           | NLTK          | spaCy         |\n",
        "| ----------------- | ------------- | ------------- |\n",
        "| Tag style         | Penn Treebank | Universal POS |\n",
        "| Detail            | Fine-grained  | Conceptual    |\n",
        "| Academic analysis | Moderate      | Strong        |\n",
        "| Readability       | Lower         | Higher        |\n"
      ],
      "metadata": {
        "id": "hF7PCo86E-oT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Extract Academic Concepts (Nouns) and Arguments (Verbs)\n",
        "Using NLTK"
      ],
      "metadata": {
        "id": "Bu_hR35tFMms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_nouns = [word for word, tag in nltk_pos_tags if tag.startswith('NN')]\n",
        "nltk_verbs = [word for word, tag in nltk_pos_tags if tag.startswith('VB')]\n"
      ],
      "metadata": {
        "id": "ROg-kh-vE0Hm"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using spaCy"
      ],
      "metadata": {
        "id": "cdTtvVocFW11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_nouns = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
        "spacy_verbs = [token.text for token in doc if token.pos_ == \"VERB\"]\n"
      ],
      "metadata": {
        "id": "aVqG9TK0FaSY"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Frequency Analysis"
      ],
      "metadata": {
        "id": "5-y-CS0nFjNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "noun_freq = Counter(spacy_nouns)\n",
        "verb_freq = Counter(spacy_verbs)\n",
        "noun_freq, verb_freq\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j1eTgM5FuPK",
        "outputId": "ceb43b0b-79d3-48f0-8baf-8a0f07c2b2f5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Counter({'writing': 1,\n",
              "          'clarity': 1,\n",
              "          'precision': 1,\n",
              "          'analysis': 1,\n",
              "          'Researchers': 1,\n",
              "          'argumentation': 1,\n",
              "          'communication': 1,\n",
              "          'study': 1,\n",
              "          'patterns': 1,\n",
              "          'discourse': 1}),\n",
              " Counter({'requires': 1, 'argue': 1, 'strengthens': 1, 'examines': 1}))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert to Table"
      ],
      "metadata": {
        "id": "bEQH8wtNF3bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_nouns = pd.DataFrame(noun_freq.items(), columns=[\"Noun\", \"Frequency\"])\n",
        "df_verbs = pd.DataFrame(verb_freq.items(), columns=[\"Verb\", \"Frequency\"])\n",
        "df_nouns, df_verbs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Nz4cB2CF4t6",
        "outputId": "be5ee9f9-e38b-492e-b34a-9ae5fc547b05"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(            Noun  Frequency\n",
              " 0        writing          1\n",
              " 1        clarity          1\n",
              " 2      precision          1\n",
              " 3       analysis          1\n",
              " 4    Researchers          1\n",
              " 5  argumentation          1\n",
              " 6  communication          1\n",
              " 7          study          1\n",
              " 8       patterns          1\n",
              " 9      discourse          1,\n",
              "           Verb  Frequency\n",
              " 0     requires          1\n",
              " 1        argue          1\n",
              " 2  strengthens          1\n",
              " 3     examines          1)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}